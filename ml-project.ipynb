{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"21fcc10e-fa0c-43e0-8761-d0ae1a985cb8","_cell_guid":"7dbacb12-58a0-4d0e-9ad3-412934815a1a","collapsed":false,"execution":{"iopub.status.busy":"2023-01-16T15:01:22.938175Z","iopub.execute_input":"2023-01-16T15:01:22.939465Z","iopub.status.idle":"2023-01-16T15:01:22.951883Z","shell.execute_reply.started":"2023-01-16T15:01:22.939415Z","shell.execute_reply":"2023-01-16T15:01:22.950483Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_ = pd.read_feather('../input/amexfeather/train_data.ftr')\n# Keep the latest statement features for each customer\ntraining_dataset = train_dataset_.groupby('customer_ID').tail(1).set_index('customer_ID', drop=True).sort_index()\n\ntest_dataset_ = pd.read_feather('/kaggle/input/amexfeather/test_data.ftr')\n# Keep the latest statement features for each customer\ntest_dataset = test_dataset_.groupby('customer_ID').tail(1).set_index('customer_ID', drop=True).sort_index()","metadata":{"_uuid":"135aec71-ac6e-47ba-98df-897d88890721","_cell_guid":"eab3fc2b-75a6-4992-b9d6-246c89f7b6a5","collapsed":false,"execution":{"iopub.status.busy":"2023-01-16T15:01:22.956667Z","iopub.execute_input":"2023-01-16T15:01:22.957038Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ndel train_dataset_\ngc.collect()","metadata":{"_uuid":"0dbc6274-3878-4973-93fb-7edbdc6ad9e9","_cell_guid":"dc71bd7e-063f-42d7-9058-d4f404c0cc50","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_dataset.head()","metadata":{"_uuid":"aed6f217-cec8-40c5-91cb-ad2d6af01bed","_cell_guid":"65802723-6f5b-42c7-a505-54292ce3e72f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntraining_dataset_cp = training_dataset.copy()\ntest_dataset_cp = test_dataset.copy()","metadata":{"_uuid":"dfc1791f-a48b-40b2-b529-59ac00a89fa9","_cell_guid":"bbf38ccd-183a-4f27-bb3e-9d777bc3fa19","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lable encoding for categoricals\nobject_cols = []\nfor colname in cat_cols.columns:\n    object_cols.append(colname)\n    training_dataset_cp[colname], _ = training_dataset_cp[colname].factorize()","metadata":{"_uuid":"ddb070b5-289c-4a8f-8c92-5e66e7cb749d","_cell_guid":"12898b66-d9fa-4a28-b457-83ceac79afcd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove columns if there are > 80% of missing values\ntraining_dataset_cp = training_dataset_cp.drop(['S_2','D_66','D_42','D_49','D_73','D_76','R_9','B_29','D_87','D_88','D_106','R_26','D_108','D_110','D_111','B_39','B_42','D_132','D_134','D_135','D_136','D_137','D_138','D_142'], axis=1)\n# Remove columns if there are > 80% of missing values\ntest_dataset_cp = test_dataset_cp.drop(['S_2','D_66','D_42','D_49','D_73','D_76','R_9','B_29','D_87','D_88','D_106','R_26','D_108','D_110','D_111','B_39','B_42','D_132','D_134','D_135','D_136','D_137','D_138','D_142'], axis=1)","metadata":{"_uuid":"e80791dc-9d15-4549-8e35-0a1c1d646b6c","_cell_guid":"e22c373c-21e9-4b54-97a6-071a6fa324bd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill with median coulmns that has null values\nselected_col = np.array(['P_2','S_3','B_2','D_41','D_43','B_3','D_44','D_45','D_46','D_48','D_50','D_53','S_7','D_56','S_9','B_6','B_8','D_52','P_3','D_54','D_55','B_13','D_59','D_61','B_15','D_62','B_16','B_17','D_77','B_19','B_20','D_69','B_22','D_70','D_72','D_74','R_7','B_25','B_26','D_78','D_79','D_80','B_27','D_81','R_12','D_82','D_105','S_27','D_83','R_14','D_84','D_86','R_20','B_33','D_89','D_91','S_22','S_23','S_24','S_25','S_26','D_102','D_103','D_104','D_107','B_37','R_27','D_109','D_112','B_40','D_113','D_115','D_118','D_119','D_121','D_122','D_123','D_124','D_125','D_128','D_129','B_41','D_130','D_131','D_133','D_139','D_140','D_141','D_143','D_144','D_145'])\n\n# fill with median coulmns that has null values\nfor col in selected_col:\n    test_dataset_cp[col] = test_dataset_cp[col].fillna(test_dataset_cp[col].median())\nfor col in selected_col:\n    training_dataset_cp[col] = training_dataset_cp[col].fillna(training_dataset_cp[col].median())","metadata":{"_uuid":"7a658480-c709-4168-8f0d-f7c466b7c980","_cell_guid":"bc390881-328e-49d0-b960-6df42c089a11","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop unusable columns\nselcted_col2 = np.array(['D_68','B_30','B_38','D_64','D_114','D_116','D_117','D_120','D_126'])\n\n# drop unusable columns\nfor col2 in selcted_col2:\n    test_dataset_cp[col2] =  test_dataset_cp[col2].fillna(test_dataset_cp[col2].mode()[0])\nfor col2 in selcted_col2:\n    training_dataset_cp[col2] =  training_dataset_cp[col2].fillna(training_dataset_cp[col2].mode()[0])","metadata":{"_uuid":"9bb5e3b9-be98-4c1b-bbb8-d9e10d3f12ed","_cell_guid":"6b32e465-493f-4a8e-a14a-4b360f2f81dc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_dataset_cp.head()","metadata":{"_uuid":"bb0309c5-9cd1-4bf9-97fe-536f13e557f0","_cell_guid":"ec2e518e-f2d3-4252-9e4b-190ca9b45124","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_target_cp = training_dataset_cp.pop(\"target\")","metadata":{"_uuid":"b2a0a555-75e6-4fb7-9bfe-3eca0ebed8b5","_cell_guid":"6314211a-ae8e-41db-9909-2f6b83704ebd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the columns with categorical data\ncat_cols = training_dataset_cp.select_dtypes(include=[\"category\"])\n\n# Print the names of the categorical columns\nprint(cat_cols.columns)\n\n# for colname in cat_cols.columns:\n#     # Get one hot encoding of columns B\n#     one_hot = pd.get_dummies(test_dataset_cp[colname], prefix=\"ohp\")\n#     # Drop column B as it is now encoded\n#     test_dataset_cp = test_dataset_cp.drop(colname,axis = 1)\n#     # Join the encoded df\n#     test_dataset_cp = test_dataset_cp.join(one_hot)\n    \n# for colname in cat_cols.columns:\n#     # Get one hot encoding of columns B\n#     one_hot = pd.get_dummies(training_dataset_cp[colname], prefix=colname)\n#     # Drop column B as it is now encoded\n#     training_dataset_cp = training_dataset_cp.drop(colname,axis = 1)\n#     # Join the encoded df\n#     training_dataset_cp = training_dataset_cp.join(one_hot)","metadata":{"_uuid":"ab0a65d2-c2af-482b-9784-d671de55668d","_cell_guid":"b1b8767d-b6b0-46af-9cf4-41bf0ce26421","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lable encoding for categoricals\nobject_cols = []\nfor colname in cat_cols.columns:\n    object_cols.append(colname)\n    training_dataset_cp[colname], _ = training_dataset_cp[colname].factorize()\nfor colname in cat_cols.columns:\n    object_cols.append(colname)\n    test_dataset_cp[colname], _ = test_dataset_cp[colname].factorize()","metadata":{"_uuid":"3d14593b-9464-43bb-aab8-34fc7e784354","_cell_guid":"8d0e463c-4bef-4ced-98e2-45489f47bacf","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(training_dataset_cp, train_target_cp, test_size=0.03,\n    shuffle=True,stratify=train_target_cp,random_state=6)\n\nscaler = StandardScaler()\n\nscaler.fit(X_train)\nscaler.transform(X_train)\n\n# scaler.fit(y_train)\n# scaler.transform(y_train)\n\nscaler.fit(X_test)\nscaler.transform(X_test)\nX_train.head()","metadata":{"_uuid":"d61cb2a5-8af4-4bc2-9edb-3b3718b70a0f","_cell_guid":"e84a8694-84f3-4b27-89d0-a0bb86aa7bf0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensembling - high training time low perfromance\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Load and pre-process data\n# X, y = load_and_preprocess_data()\n\n# Split data into training and test sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Create individual models\nlog_reg = LogisticRegression()\ndecision_tree = DecisionTreeClassifier()\nrandom_forest = RandomForestClassifier()\n\n# Create ensemble model using majority voting\nensemble = VotingClassifier(estimators=[('lr', log_reg), ('dt', decision_tree), ('rf', random_forest)], voting='soft')\n\n# Fit ensemble model on training data\nensemble.fit(X_train, y_train)\n\n# Evaluate ensemble model on test data\nensemble_accuracy = ensemble.score(X_test, y_test)\nprint(\"Ensemble model accuracy: \", ensemble_accuracy)","metadata":{"_uuid":"0c8afbf7-1729-451c-9829-08365adf70c7","_cell_guid":"b50004cc-8aed-4b8d-9985-b925ff1e6366","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlrclf = LogisticRegression(C=4,class_weight=\"balanced\",dual=False,fit_intercept=True,\n                           intercept_scaling=100,l1_ratio=None,max_iter=5,multi_class='auto',\n                           n_jobs=None,penalty='l2',random_state=None,solver='lbfgs',tol=0.0001,\n                           verbose=0,warm_start=False)\n\npred = lrclf.fit(X_train, y_train).predict_proba(X_test)\n\nprint(roc_auc_score(y_test, pred[:, 1])) # new","metadata":{"_uuid":"f5bcec7c-6150-4464-9e34-daafe726c224","_cell_guid":"5a414109-ab26-4f1f-ba21-be9a3fc9bebb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nimport optuna\n\ndef hyperparameter_tuning(trial):\n    # Define the hyperparameters to be tuned\n    params = {\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0)\n    }\n\n    # Create the LightGBM model\n    model = lgb.LGBMClassifier(**params)\n\n    # Train the model on the training data\n    model.fit(X_train, y_train)\n\n    # Predict the target values for the validation data\n    y_pred = model.predict_proba(X_test)\n\n    # Calculate the evaluation metric\n    accuracy = roc_auc_score(y_test, y_pred[:, 1])\n\n    # Report the evaluation metric to Optuna\n    return -accuracy\n\n# commented since no need to run always\n# # Create the Optuna study\n# study = optuna.create_study()\n\n# # Run the hyperparameter tuning using Optuna's optimize function\n# study.optimize(hyperparameter_tuning, n_trials=100)\n\n# # Print the best hyperparameter values and evaluation metric\n# print('Best hyperparameters:', study.best_params)\n# print('Best accuracy:', study.best_value)","metadata":{"_uuid":"cb051e67-1df6-42fc-a73c-7231c2215b78","_cell_guid":"5e7e234b-c874-4557-8e30-4a8efc37279d","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuned hyperparameters\nparams = {\n'learning_rate': 0.08395719054553268, 'max_depth': 9, 'num_leaves': 62, 'feature_fraction': 0.5354486758502452\n}\n\n# Create the LightGBM model\nmodel_lgbm = lgb.LGBMClassifier(**params)\n\n# Train the model on the training data\nmodel_lgbm.fit(X_train, y_train)\n\n# Predict the target values for the validation data\ny_pred = model_lgbm.predict_proba(X_test)\n\nprint(roc_auc_score(y_test, y_pred[:, 1])) # new","metadata":{"_uuid":"3a14dd2d-25aa-4d3e-886d-c67180fc0fc3","_cell_guid":"17529481-d6c3-4316-91ff-eba6e5971356","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SVM - not enough perfromance and high training time\nimport optuna\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\ndef objective(trial):\n    # extract the hyperparameters\n    C = trial.suggest_float('C', 0.1, 10)\n    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf'])\n    gamma = trial.suggest_float('gamma', 0.1, 10)\n    # create the SVM model\n    svm = SVC(C=C, kernel=kernel, gamma=gamma)\n    # use cross-validation to evaluate the model\n    return -1.0 * cross_val_score(svm, X_train, y_train, cv=5).mean()\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\n\n# print the best parameters\nprint(\"Best params: \", study.best_params)","metadata":{"_uuid":"cf7cfc11-ce4e-4b26-8316-c8a00f0639f6","_cell_guid":"2a5904ab-0aac-4c57-aa81-d33d1846626b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\n\nd_train = lgb.Dataset(X_train, label=y_train)\n\nparams = {'objective': 'binary','n_estimators': 1200,'metric': 'binary_logloss','boosting': 'gbdt','num_leaves': 90,'reg_lambda' : 50,'colsample_bytree': 0.19,'learning_rate': 0.03,'min_child_samples': 2400,'max_bins': 511,'seed': 42,'verbose': -1}\n\n# trained model with 100 iterations\nmodel_lgb = lgb.train(params, d_train, 100)","metadata":{"_uuid":"e0028dc5-3f33-4a98-868c-716fe91e222b","_cell_guid":"a4a75d1e-b0f6-4ec5-bfa0-ea6648606391","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\n\nmodel_cboost = CatBoostClassifier(iterations=1000,\n                           task_type=\"CPU\",\n                           devices='0:1')\nmodel_cboost.fit(X_train,\n          y_train,\n          verbose=True)","metadata":{"_uuid":"a881a0b7-679b-4d87-8b93-7686d599aea2","_cell_guid":"23ad368b-664e-423e-bc87-9873480222e3","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model_cboost.predict_proba(X_test)\nprint(roc_auc_score(y_test, y_pred[:, 1])) # new","metadata":{"_uuid":"82ac339d-24a6-4159-9f3b-f422c9964dad","_cell_guid":"0c514cf1-1692-44e8-bb43-6c347605216d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\n\nKNN = KNeighborsClassifier()\nclf = KNN.fit(X_train, y_train)\npred = clf.predict_proba(X_test)\n# not enough performance in KNN","metadata":{"_uuid":"8701b29c-5e52-4952-8ab7-4a47040cff9a","_cell_guid":"6d9ae6c2-8923-4f28-8776-a109b3248a06","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filling null values with mode in test_dataset\ncolumnsWithNa = test_dataset_cp.columns[test_dataset_cp.isnull().any()].tolist()\nfor column in columnsWithNa :\n    test_dataset_cp[column].fillna(test_dataset_cp[column].mode()[0], inplace = True)","metadata":{"_uuid":"609f6296-f848-44a5-b46c-c0b32a22d3be","_cell_guid":"e82ac08c-8a9a-4df2-a380-645cb70d4af3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred = pd.DataFrame({\"prediction\": lrclf.predict_proba(test_dataset_cp)[:, 1]}, index=test_dataset_cp.index)\n# # del y_test['customer_ID']\n# pred.to_csv(\"submission.csv\")\n# pred.head()","metadata":{"_uuid":"298c127e-b5b3-42bb-86b0-a869200810e7","_cell_guid":"e26dc181-338e-482d-b5ab-4b9d9e3e4256","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions = ensemble.predict_proba(test_dataset_cp)","metadata":{"_uuid":"6a2a3373-0c6b-41ad-9dc7-069b61817ada","_cell_guid":"cc12561c-520d-498b-8390-2fdf3837279d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get presictions for submission\npredictions = model_cboost.predict_proba(test_dataset_cp)\nprint(predictions)","metadata":{"_uuid":"8885bf0f-ebb4-450b-8ff4-2448c5d5a0eb","_cell_guid":"8c304d5f-79b9-49e2-a59a-64e0f4993e64","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'customer_ID': test_dataset_cp.index, 'prediction': predictions[:,1]})\noutput.to_csv('submission.csv', index=False)","metadata":{"_uuid":"47bc8781-b257-45d5-a24e-daaa508c8c09","_cell_guid":"1289c204-d21f-42d5-8893-eea34f5dae5c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}